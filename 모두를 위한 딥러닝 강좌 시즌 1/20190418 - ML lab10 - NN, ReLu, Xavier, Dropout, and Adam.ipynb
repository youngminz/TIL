{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Ke_89fC4aYA_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1768
        },
        "outputId": "9dc808ad-6fc9-4c62-b604-ebc6defc49e0"
      },
      "cell_type": "code",
      "source": [
        "# Lab 7 Learning rate and Evaluation\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "tf.set_random_seed(777)  # reproducibility\n",
        "\n",
        "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
        "# more information about the mnist dataset\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "# input place holders\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# weights & bias for nn layers\n",
        "W = tf.Variable(tf.random_normal([784, 10]))\n",
        "b = tf.Variable(tf.random_normal([10]))\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.001\n",
        "batch_size = 100\n",
        "num_epochs = 50\n",
        "num_iterations = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "hypothesis = tf.matmul(X, W) + b\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "cost = tf.reduce_mean(\n",
        "    tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "        logits=hypothesis, labels=tf.stop_gradient(Y)\n",
        "    )\n",
        ")\n",
        "train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(hypothesis, axis=1), tf.argmax(Y, axis=1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "# train my model\n",
        "with tf.Session() as sess:\n",
        "    # initialize\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        avg_cost = 0\n",
        "\n",
        "        for iteration in range(num_iterations):\n",
        "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "            _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
        "            avg_cost += cost_val / num_iterations\n",
        "\n",
        "        print(f\"Epoch: {(epoch + 1):04d}, Cost: {avg_cost:.9f}\")\n",
        "\n",
        "    print(\"Learning Finished!\")\n",
        "\n",
        "    # Test model and check accuracy\n",
        "    print(\n",
        "        \"Accuracy:\",\n",
        "        sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels}),\n",
        "    )\n",
        "\n",
        "    # Get one and predict\n",
        "    r = random.randint(0, mnist.test.num_examples - 1)\n",
        "\n",
        "    print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r : r + 1], axis=1)))\n",
        "    print(\n",
        "        \"Prediction: \",\n",
        "        sess.run(\n",
        "            tf.argmax(hypothesis, axis=1), feed_dict={X: mnist.test.images[r : r + 1]}\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    plt.imshow(\n",
        "        mnist.test.images[r : r + 1].reshape(28, 28),\n",
        "        cmap=\"Greys\",\n",
        "        interpolation=\"nearest\",\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "'''\n",
        "Epoch: 0001 Cost: 5.745170949\n",
        "Epoch: 0002 Cost: 1.780056722\n",
        "Epoch: 0003 Cost: 1.122778654\n",
        "...\n",
        "Epoch: 0048 Cost: 0.271918680\n",
        "Epoch: 0049 Cost: 0.270640434\n",
        "Epoch: 0050 Cost: 0.269054370\n",
        "Learning Finished!\n",
        "Accuracy: 0.9194\n",
        "'''"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-c6b7351a2ba1>:11: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use urllib or similar directly.\n",
            "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "Epoch: 0001, Cost: 5.745170968\n",
            "Epoch: 0002, Cost: 1.780056713\n",
            "Epoch: 0003, Cost: 1.122778642\n",
            "Epoch: 0004, Cost: 0.872012252\n",
            "Epoch: 0005, Cost: 0.738203198\n",
            "Epoch: 0006, Cost: 0.654728902\n",
            "Epoch: 0007, Cost: 0.596023617\n",
            "Epoch: 0008, Cost: 0.552216836\n",
            "Epoch: 0009, Cost: 0.518254969\n",
            "Epoch: 0010, Cost: 0.491113210\n",
            "Epoch: 0011, Cost: 0.468347541\n",
            "Epoch: 0012, Cost: 0.449374360\n",
            "Epoch: 0013, Cost: 0.432675665\n",
            "Epoch: 0014, Cost: 0.418828162\n",
            "Epoch: 0015, Cost: 0.406128942\n",
            "Epoch: 0016, Cost: 0.394982949\n",
            "Epoch: 0017, Cost: 0.385870423\n",
            "Epoch: 0018, Cost: 0.376135585\n",
            "Epoch: 0019, Cost: 0.368269379\n",
            "Epoch: 0020, Cost: 0.361209779\n",
            "Epoch: 0021, Cost: 0.354798146\n",
            "Epoch: 0022, Cost: 0.348525123\n",
            "Epoch: 0023, Cost: 0.342752727\n",
            "Epoch: 0024, Cost: 0.337285914\n",
            "Epoch: 0025, Cost: 0.332443598\n",
            "Epoch: 0026, Cost: 0.327556537\n",
            "Epoch: 0027, Cost: 0.324047233\n",
            "Epoch: 0028, Cost: 0.319670903\n",
            "Epoch: 0029, Cost: 0.315536207\n",
            "Epoch: 0030, Cost: 0.312257761\n",
            "Epoch: 0031, Cost: 0.308550810\n",
            "Epoch: 0032, Cost: 0.305987609\n",
            "Epoch: 0033, Cost: 0.302624457\n",
            "Epoch: 0034, Cost: 0.299895901\n",
            "Epoch: 0035, Cost: 0.297245874\n",
            "Epoch: 0036, Cost: 0.294490167\n",
            "Epoch: 0037, Cost: 0.292061209\n",
            "Epoch: 0038, Cost: 0.290009239\n",
            "Epoch: 0039, Cost: 0.287633526\n",
            "Epoch: 0040, Cost: 0.285644496\n",
            "Epoch: 0041, Cost: 0.283856600\n",
            "Epoch: 0042, Cost: 0.281824816\n",
            "Epoch: 0043, Cost: 0.280098973\n",
            "Epoch: 0044, Cost: 0.278386738\n",
            "Epoch: 0045, Cost: 0.276589557\n",
            "Epoch: 0046, Cost: 0.275093699\n",
            "Epoch: 0047, Cost: 0.273444049\n",
            "Epoch: 0048, Cost: 0.271918682\n",
            "Epoch: 0049, Cost: 0.270640435\n",
            "Epoch: 0050, Cost: 0.269054375\n",
            "Learning Finished!\n",
            "Accuracy: 0.9194\n",
            "Label:  [3]\n",
            "Prediction:  [3]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wcVzCZEnx/s5AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nEpoch: 0001 Cost: 5.745170949\\nEpoch: 0002 Cost: 1.780056722\\nEpoch: 0003 Cost: 1.122778654\\n...\\nEpoch: 0048 Cost: 0.271918680\\nEpoch: 0049 Cost: 0.270640434\\nEpoch: 0050 Cost: 0.269054370\\nLearning Finished!\\nAccuracy: 0.9194\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "jPXw3cr01Yoa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "5aa1c585-65e8-4605-f89b-ce36213b3c2b"
      },
      "cell_type": "code",
      "source": [
        "# Lab 10 MNIST and NN\n",
        "import tensorflow as tf\n",
        "import random\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "tf.set_random_seed(777)  # reproducibility\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
        "# more information about the mnist dataset\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "# input place holders\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# weights & bias for nn layers\n",
        "W1 = tf.Variable(tf.random_normal([784, 256]))\n",
        "b1 = tf.Variable(tf.random_normal([256]))\n",
        "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "\n",
        "W2 = tf.Variable(tf.random_normal([256, 256]))\n",
        "b2 = tf.Variable(tf.random_normal([256]))\n",
        "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
        "\n",
        "W3 = tf.Variable(tf.random_normal([256, 10]))\n",
        "b3 = tf.Variable(tf.random_normal([10]))\n",
        "hypothesis = tf.matmul(L2, W3) + b3\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=hypothesis, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initialize\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# train my model\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
        "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
        "        avg_cost += c / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning Finished!')\n",
        "\n",
        "# Test model and check accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
        "      X: mnist.test.images, Y: mnist.test.labels}))\n",
        "\n",
        "# Get one and predict\n",
        "r = random.randint(0, mnist.test.num_examples - 1)\n",
        "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
        "print(\"Prediction: \", sess.run(\n",
        "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
        "\n",
        "# plt.imshow(mnist.test.images[r:r + 1].\n",
        "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
        "# plt.show()\n",
        "\n",
        "'''\n",
        "Epoch: 0001 cost = 141.207671860\n",
        "Epoch: 0002 cost = 38.788445864\n",
        "Epoch: 0003 cost = 23.977515479\n",
        "Epoch: 0004 cost = 16.315132428\n",
        "Epoch: 0005 cost = 11.702554882\n",
        "Epoch: 0006 cost = 8.573139748\n",
        "Epoch: 0007 cost = 6.370995680\n",
        "Epoch: 0008 cost = 4.537178684\n",
        "Epoch: 0009 cost = 3.216900532\n",
        "Epoch: 0010 cost = 2.329708954\n",
        "Epoch: 0011 cost = 1.715552875\n",
        "Epoch: 0012 cost = 1.189857912\n",
        "Epoch: 0013 cost = 0.820965160\n",
        "Epoch: 0014 cost = 0.624131458\n",
        "Epoch: 0015 cost = 0.454633765\n",
        "Learning Finished!\n",
        "Accuracy: 0.9455\n",
        "'''"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From <ipython-input-3-d358a9b9373f>:37: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Epoch: 0001 cost = 166.216929796\n",
            "Epoch: 0002 cost = 41.074498711\n",
            "Epoch: 0003 cost = 25.669349086\n",
            "Epoch: 0004 cost = 17.948888205\n",
            "Epoch: 0005 cost = 13.000822711\n",
            "Epoch: 0006 cost = 9.687766329\n",
            "Epoch: 0007 cost = 7.240173408\n",
            "Epoch: 0008 cost = 5.570820060\n",
            "Epoch: 0009 cost = 4.184354778\n",
            "Epoch: 0010 cost = 3.166544509\n",
            "Epoch: 0011 cost = 2.284875372\n",
            "Epoch: 0012 cost = 1.644473258\n",
            "Epoch: 0013 cost = 1.286292889\n",
            "Epoch: 0014 cost = 0.991245993\n",
            "Epoch: 0015 cost = 0.769280673\n",
            "Learning Finished!\n",
            "Accuracy: 0.9419\n",
            "Label:  [5]\n",
            "Prediction:  [5]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nEpoch: 0001 cost = 141.207671860\\nEpoch: 0002 cost = 38.788445864\\nEpoch: 0003 cost = 23.977515479\\nEpoch: 0004 cost = 16.315132428\\nEpoch: 0005 cost = 11.702554882\\nEpoch: 0006 cost = 8.573139748\\nEpoch: 0007 cost = 6.370995680\\nEpoch: 0008 cost = 4.537178684\\nEpoch: 0009 cost = 3.216900532\\nEpoch: 0010 cost = 2.329708954\\nEpoch: 0011 cost = 1.715552875\\nEpoch: 0012 cost = 1.189857912\\nEpoch: 0013 cost = 0.820965160\\nEpoch: 0014 cost = 0.624131458\\nEpoch: 0015 cost = 0.454633765\\nLearning Finished!\\nAccuracy: 0.9455\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "YC4hCNaJ1nuY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "f2aa44bb-7412-42e7-a4f3-0976a94082bc"
      },
      "cell_type": "code",
      "source": [
        "# Lab 10 MNIST and Xavier\n",
        "import tensorflow as tf\n",
        "import random\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "tf.set_random_seed(777)  # reproducibility\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
        "# more information about the mnist dataset\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "# input place holders\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# weights & bias for nn layers\n",
        "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
        "W1 = tf.get_variable(\"W1\", shape=[784, 256],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b1 = tf.Variable(tf.random_normal([256]))\n",
        "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "\n",
        "W2 = tf.get_variable(\"W2\", shape=[256, 256],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b2 = tf.Variable(tf.random_normal([256]))\n",
        "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
        "\n",
        "W3 = tf.get_variable(\"W3\", shape=[256, 10],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b3 = tf.Variable(tf.random_normal([10]))\n",
        "hypothesis = tf.matmul(L2, W3) + b3\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=hypothesis, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initialize\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# train my model\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
        "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
        "        avg_cost += c / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning Finished!')\n",
        "\n",
        "# Test model and check accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
        "      X: mnist.test.images, Y: mnist.test.labels}))\n",
        "\n",
        "# Get one and predict\n",
        "r = random.randint(0, mnist.test.num_examples - 1)\n",
        "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
        "print(\"Prediction: \", sess.run(\n",
        "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
        "\n",
        "# plt.imshow(mnist.test.images[r:r + 1].\n",
        "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
        "# plt.show()\n",
        "\n",
        "'''\n",
        "Epoch: 0001 cost = 0.301498963\n",
        "Epoch: 0002 cost = 0.107252513\n",
        "Epoch: 0003 cost = 0.064888892\n",
        "Epoch: 0004 cost = 0.044463030\n",
        "Epoch: 0005 cost = 0.029951642\n",
        "Epoch: 0006 cost = 0.020663404\n",
        "Epoch: 0007 cost = 0.015853033\n",
        "Epoch: 0008 cost = 0.011764387\n",
        "Epoch: 0009 cost = 0.008598264\n",
        "Epoch: 0010 cost = 0.007383116\n",
        "Epoch: 0011 cost = 0.006839140\n",
        "Epoch: 0012 cost = 0.004672963\n",
        "Epoch: 0013 cost = 0.003979437\n",
        "Epoch: 0014 cost = 0.002714260\n",
        "Epoch: 0015 cost = 0.004707661\n",
        "Learning Finished!\n",
        "Accuracy: 0.9783\n",
        "'''"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "Epoch: 0001 cost = 0.302051391\n",
            "Epoch: 0002 cost = 0.120358052\n",
            "Epoch: 0003 cost = 0.077937337\n",
            "Epoch: 0004 cost = 0.056028680\n",
            "Epoch: 0005 cost = 0.041008632\n",
            "Epoch: 0006 cost = 0.032491862\n",
            "Epoch: 0007 cost = 0.025326364\n",
            "Epoch: 0008 cost = 0.020288483\n",
            "Epoch: 0009 cost = 0.016832824\n",
            "Epoch: 0010 cost = 0.015247146\n",
            "Epoch: 0011 cost = 0.011590952\n",
            "Epoch: 0012 cost = 0.012914067\n",
            "Epoch: 0013 cost = 0.009365288\n",
            "Epoch: 0014 cost = 0.012037884\n",
            "Epoch: 0015 cost = 0.008060220\n",
            "Learning Finished!\n",
            "Accuracy: 0.9763\n",
            "Label:  [1]\n",
            "Prediction:  [1]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nEpoch: 0001 cost = 0.301498963\\nEpoch: 0002 cost = 0.107252513\\nEpoch: 0003 cost = 0.064888892\\nEpoch: 0004 cost = 0.044463030\\nEpoch: 0005 cost = 0.029951642\\nEpoch: 0006 cost = 0.020663404\\nEpoch: 0007 cost = 0.015853033\\nEpoch: 0008 cost = 0.011764387\\nEpoch: 0009 cost = 0.008598264\\nEpoch: 0010 cost = 0.007383116\\nEpoch: 0011 cost = 0.006839140\\nEpoch: 0012 cost = 0.004672963\\nEpoch: 0013 cost = 0.003979437\\nEpoch: 0014 cost = 0.002714260\\nEpoch: 0015 cost = 0.004707661\\nLearning Finished!\\nAccuracy: 0.9783\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "sWJmpF611_nx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        },
        "outputId": "7d600136-c7c9-4959-895c-53eed4db4a5d"
      },
      "cell_type": "code",
      "source": [
        "# Lab 10 MNIST and Deep learning\n",
        "import tensorflow as tf\n",
        "import random\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "tf.set_random_seed(777)  # reproducibility\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
        "# more information about the mnist dataset\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "# input place holders\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# weights & bias for nn layers\n",
        "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
        "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b1 = tf.Variable(tf.random_normal([512]))\n",
        "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "\n",
        "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b2 = tf.Variable(tf.random_normal([512]))\n",
        "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
        "\n",
        "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b3 = tf.Variable(tf.random_normal([512]))\n",
        "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
        "\n",
        "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([512]))\n",
        "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
        "\n",
        "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b5 = tf.Variable(tf.random_normal([10]))\n",
        "hypothesis = tf.matmul(L4, W5) + b5\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=hypothesis, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initialize\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# train my model\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "        feed_dict = {X: batch_xs, Y: batch_ys}\n",
        "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
        "        avg_cost += c / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning Finished!')\n",
        "\n",
        "# Test model and check accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
        "      X: mnist.test.images, Y: mnist.test.labels}))\n",
        "\n",
        "# Get one and predict\n",
        "r = random.randint(0, mnist.test.num_examples - 1)\n",
        "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
        "print(\"Prediction: \", sess.run(\n",
        "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1]}))\n",
        "\n",
        "# plt.imshow(mnist.test.images[r:r + 1].\n",
        "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
        "# plt.show()\n",
        "\n",
        "'''\n",
        "Epoch: 0001 cost = 0.266061549\n",
        "Epoch: 0002 cost = 0.080796588\n",
        "Epoch: 0003 cost = 0.049075800\n",
        "Epoch: 0004 cost = 0.034772298\n",
        "Epoch: 0005 cost = 0.024780529\n",
        "Epoch: 0006 cost = 0.017072763\n",
        "Epoch: 0007 cost = 0.014031383\n",
        "Epoch: 0008 cost = 0.013763446\n",
        "Epoch: 0009 cost = 0.009164047\n",
        "Epoch: 0010 cost = 0.008291388\n",
        "Epoch: 0011 cost = 0.007319742\n",
        "Epoch: 0012 cost = 0.006434021\n",
        "Epoch: 0013 cost = 0.005684378\n",
        "Epoch: 0014 cost = 0.004781207\n",
        "Epoch: 0015 cost = 0.004342310\n",
        "Learning Finished!\n",
        "Accuracy: 0.9742\n",
        "'''"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-8a6150407670>:9: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-1-8a6150407670>:51: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Epoch: 0001 cost = 0.296395403\n",
            "Epoch: 0002 cost = 0.105863225\n",
            "Epoch: 0003 cost = 0.071295454\n",
            "Epoch: 0004 cost = 0.052339334\n",
            "Epoch: 0005 cost = 0.040581879\n",
            "Epoch: 0006 cost = 0.037883751\n",
            "Epoch: 0007 cost = 0.030099787\n",
            "Epoch: 0008 cost = 0.027994805\n",
            "Epoch: 0009 cost = 0.021352890\n",
            "Epoch: 0010 cost = 0.019132408\n",
            "Epoch: 0011 cost = 0.020569148\n",
            "Epoch: 0012 cost = 0.019067912\n",
            "Epoch: 0013 cost = 0.015929284\n",
            "Epoch: 0014 cost = 0.015690700\n",
            "Epoch: 0015 cost = 0.016600471\n",
            "Learning Finished!\n",
            "Accuracy: 0.9767\n",
            "Label:  [8]\n",
            "Prediction:  [8]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nEpoch: 0001 cost = 0.266061549\\nEpoch: 0002 cost = 0.080796588\\nEpoch: 0003 cost = 0.049075800\\nEpoch: 0004 cost = 0.034772298\\nEpoch: 0005 cost = 0.024780529\\nEpoch: 0006 cost = 0.017072763\\nEpoch: 0007 cost = 0.014031383\\nEpoch: 0008 cost = 0.013763446\\nEpoch: 0009 cost = 0.009164047\\nEpoch: 0010 cost = 0.008291388\\nEpoch: 0011 cost = 0.007319742\\nEpoch: 0012 cost = 0.006434021\\nEpoch: 0013 cost = 0.005684378\\nEpoch: 0014 cost = 0.004781207\\nEpoch: 0015 cost = 0.004342310\\nLearning Finished!\\nAccuracy: 0.9742\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "ZTKfVGgN2dPE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "outputId": "2cd5dfb5-4996-4071-f29e-7c891d76d6c1"
      },
      "cell_type": "code",
      "source": [
        "# Lab 10 MNIST and Dropout\n",
        "import tensorflow as tf\n",
        "import random\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "tf.set_random_seed(777)  # reproducibility\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "# Check out https://www.tensorflow.org/get_started/mnist/beginners for\n",
        "# more information about the mnist dataset\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "total_batch = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "# input place holders\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# dropout (keep_prob) rate  0.7 on training, but should be 1 for testing\n",
        "keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "# weights & bias for nn layers\n",
        "# http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow\n",
        "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b1 = tf.Variable(tf.random_normal([512]))\n",
        "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "\n",
        "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b2 = tf.Variable(tf.random_normal([512]))\n",
        "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
        "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
        "\n",
        "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b3 = tf.Variable(tf.random_normal([512]))\n",
        "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
        "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
        "\n",
        "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([512]))\n",
        "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
        "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
        "\n",
        "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b5 = tf.Variable(tf.random_normal([10]))\n",
        "hypothesis = tf.matmul(L4, W5) + b5\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=hypothesis, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initialize\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# train my model\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
        "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
        "        avg_cost += c / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning Finished!')\n",
        "\n",
        "# Test model and check accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
        "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n",
        "\n",
        "# Get one and predict\n",
        "r = random.randint(0, mnist.test.num_examples - 1)\n",
        "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
        "print(\"Prediction: \", sess.run(\n",
        "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
        "\n",
        "# plt.imshow(mnist.test.images[r:r + 1].\n",
        "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
        "# plt.show()\n",
        "\n",
        "'''\n",
        "Epoch: 0001 cost = 0.447322626\n",
        "Epoch: 0002 cost = 0.157285590\n",
        "Epoch: 0003 cost = 0.121884535\n",
        "Epoch: 0004 cost = 0.098128681\n",
        "Epoch: 0005 cost = 0.082901778\n",
        "Epoch: 0006 cost = 0.075337573\n",
        "Epoch: 0007 cost = 0.069752543\n",
        "Epoch: 0008 cost = 0.060884363\n",
        "Epoch: 0009 cost = 0.055276413\n",
        "Epoch: 0010 cost = 0.054631256\n",
        "Epoch: 0011 cost = 0.049675195\n",
        "Epoch: 0012 cost = 0.049125314\n",
        "Epoch: 0013 cost = 0.047231930\n",
        "Epoch: 0014 cost = 0.041290121\n",
        "Epoch: 0015 cost = 0.043621063\n",
        "Learning Finished!\n",
        "Accuracy: 0.9804\n",
        "'''"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-d9fc9e001842>:9: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-1-d9fc9e001842>:32: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-1-d9fc9e001842>:59: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Epoch: 0001 cost = 0.464354967\n",
            "Epoch: 0002 cost = 0.171366879\n",
            "Epoch: 0003 cost = 0.130513366\n",
            "Epoch: 0004 cost = 0.107036653\n",
            "Epoch: 0005 cost = 0.092129551\n",
            "Epoch: 0006 cost = 0.080593342\n",
            "Epoch: 0007 cost = 0.074674249\n",
            "Epoch: 0008 cost = 0.068393815\n",
            "Epoch: 0009 cost = 0.063238925\n",
            "Epoch: 0010 cost = 0.060233094\n",
            "Epoch: 0011 cost = 0.059482499\n",
            "Epoch: 0012 cost = 0.052521904\n",
            "Epoch: 0013 cost = 0.050202670\n",
            "Epoch: 0014 cost = 0.047947720\n",
            "Epoch: 0015 cost = 0.047159635\n",
            "Learning Finished!\n",
            "Accuracy: 0.9823\n",
            "Label:  [4]\n",
            "Prediction:  [4]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nEpoch: 0001 cost = 0.447322626\\nEpoch: 0002 cost = 0.157285590\\nEpoch: 0003 cost = 0.121884535\\nEpoch: 0004 cost = 0.098128681\\nEpoch: 0005 cost = 0.082901778\\nEpoch: 0006 cost = 0.075337573\\nEpoch: 0007 cost = 0.069752543\\nEpoch: 0008 cost = 0.060884363\\nEpoch: 0009 cost = 0.055276413\\nEpoch: 0010 cost = 0.054631256\\nEpoch: 0011 cost = 0.049675195\\nEpoch: 0012 cost = 0.049125314\\nEpoch: 0013 cost = 0.047231930\\nEpoch: 0014 cost = 0.041290121\\nEpoch: 0015 cost = 0.043621063\\nLearning Finished!\\nAccuracy: 0.9804\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "j2h2kZvF3bXj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "여러 옵티마이저가 있음\n",
        "\n",
        "ADAM이라는 옵티마이저는 상당히 좋은 결과를 내어 줌\n",
        "\n",
        "처음에 시작을 ADAM으로 사용하면 됨\n",
        "\n",
        "GradientDecentOptimizer 대신 그대로 AdamOptimizer 사용하면 됨"
      ]
    },
    {
      "metadata": {
        "id": "ltC7C4ZH3iKX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}