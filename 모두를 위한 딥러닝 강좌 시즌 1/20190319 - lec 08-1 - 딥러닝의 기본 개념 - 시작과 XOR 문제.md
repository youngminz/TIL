# 20190319 - lec 08-1 - 딥러닝의 기본 개념: 시작과 XOR 문제

- 인류의 궁극적인 꿈은 생각하는 기계를 만드는 것임
- 사실 뇌는 굉장히 복잡하게 연결 되어있음
- 뉴런이라고 불리는 뉴런이 단순하게 동작하더라

## Activation Function
- x 신호가 들어오면 어떤 값으로 곱한 값과 bias를 합한 결과가 activation function를 통과할 때 값을 넘겨준다
- 어떤 값보다 크면 1, 작으면 0

## Logistic Regrssion Units
- 여러 개의 출력을 동시에 낼 수 있는 기계를 만들어 낼 수 있음
- 57년도에 선을 직접 이어서 만들었음
- 60년도에는 학습을 위하여 다이얼을 돌려가면서 연구 했음

## False Promise - 허황된 약속
- 이것이 곧 스스로 학습해서 걷고, 말하고, 보고, 쓰고, 스스로 만들어내고, 자기의 존재를 인식하게 될 것이다.
- 현재도 이루어지고 있지 않음

## (Simple) XOR problem: linearly separable?
- AND/OR 로직을 선 한 개를 그어서 풀 수 있다
- XOR 문제는 선 한 개로는 어떻게든 풀 수 없음. 50% 밖에 안 된다.

## Perceptrons (1969)
- XOR은 풀 수 없다고 수학적인 증명을 해 버림
- 여러 개를 합치면 (MLP, Multilayer perceptrons) 가능하지만, 각각의 w, b를 학습시킬 수 없음
- 많은 사람들이 실망함
- 지구 상의 아무도 학습할 방법을 찾지 못할 것이다 - Marvin Minsky, 1969
- 뉴럴 네트워크의 발전은 10년 ~ 20년 퇴보하게 됨

## Backpropagation (1974, 1982 by Paul Werbos, 1986 by Hinton (재발견))
- XOR 문제가 해결됨 (전체를 미분하는 방법을 모르고 있었는데 하나의 w를 한단계씩 조절하는 방법을 알아냄)
- 네트워크의 출력이 틀릴 경우 앞으로 수정하는 게 아닌 뒤로 수정시키면 어떨까 하는 방법을 제안함
- 각각의 w, b가 있을 때 틀린 정보면 조절하기 어렵다는 문제 해결

## Convolutional Neural Networks
- 고양이에게 그림을 그려주고 뇌의 뉴런을 확인했는데, 각각의 그림마다 활성화되는 뉴런이 다름을 발견
- 쪼개고, 쪼개고, 다시 쪼갠 다음 다시 합쳐서 사용됨
- 90% 이상의 정확도
- 84년부터 94년까지 자율주행차를 만들어 어느 정도 성능을 냄

## BIG Problem
- Backpropagation은 몇 개 정도의 레이어에서는 잘 동작하는데, 10여 개 이상의 레이어를 학습시킬 경우 잘 안 됨
- 앞에 있는 에러를 뒤로 보낼 때 에러가 거의 전달되지 않음
- 뉴럴 네트워크 보다 더 간단한 알고리즘 (SVM, RandomForest 등)이 더 잘 동작하더라
- 뉴럴 네트워크의 두 번째 침체기에 들어감
