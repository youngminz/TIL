# 20190415 - lec10-2: Weight 초기화 잘해보자

- ReLU를 사용하는데도 실행 시킬 때마다 Cost 함수가 달라지는 경우가 생김
- 이는 랜덤 값에 따라 차이가 나게 됨
    - 초기 값을 0으로 두어 보았더니, Chain Rule에서 기울기가 0이 되어 Gradient가 사라짐
    - 초기 값을 잘 두는 것이 중요함!

## Restricted Boatman Machine (RBM) - 2006
- 입력 -> 출력 -> 입력으로 왔을 때 최대한 비슷한 값으로 만듬
- 아이디어: 레이어가 많을 때 초기 값이 준 값과 유사하게 나오는 것
- Deep Belief Network라고 함
- 학습이라고 하지 않고 Fine Tuning이라고 했음
- 좋은 소식: RBM을 사용하지 않아도 됨
    - 너무 작지도 않고, 너무 크지도 않게 적당하게 주면 됨
    - Xavier initialization: `W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in)`
    - He initialization: `W = np.random.randn(fan_in, fan_out) / np.sqrt(fan_in / 2)`
    - 실제로 실습을 해 보면 아주 잘 되는 것을 알 수 있음
- 아직 초기화를 어떻게 해야 하는지는 모름
